{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 9: Predicting Forest Cover Type with SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Eva Newby\n",
    "\n",
    "Collaborators: Emma Bea Mitchell, Michelle Yiv\n",
    "\n",
    "### Introduction\n",
    "In this lab, we will explore the application of Support Vector Machines (SVMs) and Random Forests (RFs) for multi-class classification using cartographic variables. Specifically, we will predict forest cover type based on a variety of environmental features such as elevation, soil type, and land aspect.\n",
    "\n",
    "Understanding forest cover classification is crucial for natural resource management. Land managers and conservationists rely on accurate predictions of vegetation types to make informed decisions about wildlife habitats, fire management, and sustainable forestry practices. However, direct field assessments of forest cover can be costly and time-consuming, making predictive models a valuable tool for estimating cover types in large or inaccessible regions.\n",
    "\n",
    "Dataset info here: https://archive.ics.uci.edu/dataset/31/covertype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"/courses/EDS232/Data/covtype_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preprocessing \n",
    "\n",
    "Before building our classification models, we need to prepare the dataset by separating the features target variable (`Cover_Type`) and  splitting the data into training and test sets. \n",
    "\n",
    "We didn't explicitly discuss it in lecture, but SVMs are sensitive to feature scale.  Use `describe()` to summarize the dataset.  Do you see anything that would require scaling of the data?  If so, apply that transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Elevation        Aspect         Slope  \\\n",
      "count  10000.000000  10000.000000  10000.000000   \n",
      "mean    2955.599500    154.450000     14.114700   \n",
      "std      281.786673    111.851861      7.499705   \n",
      "min     1860.000000      0.000000      0.000000   \n",
      "25%     2804.750000     58.000000      9.000000   \n",
      "50%     2995.000000    126.000000     13.000000   \n",
      "75%     3159.000000    258.000000     18.000000   \n",
      "max     3846.000000    359.000000     65.000000   \n",
      "\n",
      "       Horizontal_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
      "count                      10000.000000                     10000.000000   \n",
      "mean                         268.097600                        45.755300   \n",
      "std                          211.899673                        58.034207   \n",
      "min                            0.000000                      -164.000000   \n",
      "25%                           95.000000                         7.000000   \n",
      "50%                          218.000000                        29.000000   \n",
      "75%                          384.000000                        68.000000   \n",
      "max                         1243.000000                       427.000000   \n",
      "\n",
      "       Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
      "count   10000.000000     10000.00000   10000.000000   \n",
      "mean     2319.360300       212.19660     223.113500   \n",
      "std      1548.558651        26.98846      19.871067   \n",
      "min         0.000000        68.00000      71.000000   \n",
      "25%      1091.750000       198.00000     213.000000   \n",
      "50%      1977.000000       218.00000     226.000000   \n",
      "75%      3279.000000       231.00000     237.000000   \n",
      "max      7078.000000       254.00000     254.000000   \n",
      "\n",
      "       Horizontal_Distance_To_Fire_Points  Wilderness_Area_Rawah  ...  \\\n",
      "count                        10000.000000           10000.000000  ...   \n",
      "mean                           142.243800            1960.040200  ...   \n",
      "std                             37.799752            1320.535941  ...   \n",
      "min                              0.000000               0.000000  ...   \n",
      "25%                            119.000000            1006.000000  ...   \n",
      "50%                            142.000000            1699.000000  ...   \n",
      "75%                            168.000000            2524.000000  ...   \n",
      "max                            246.000000            7111.000000  ...   \n",
      "\n",
      "       Soil_Type_32  Soil_Type_33  Soil_Type_34  Soil_Type_35  Soil_Type_36  \\\n",
      "count  10000.000000  10000.000000   10000.00000   10000.00000  10000.000000   \n",
      "mean       0.091200      0.082900       0.00250       0.00250      0.000200   \n",
      "std        0.287908      0.275745       0.04994       0.04994      0.014141   \n",
      "min        0.000000      0.000000       0.00000       0.00000      0.000000   \n",
      "25%        0.000000      0.000000       0.00000       0.00000      0.000000   \n",
      "50%        0.000000      0.000000       0.00000       0.00000      0.000000   \n",
      "75%        0.000000      0.000000       0.00000       0.00000      0.000000   \n",
      "max        1.000000      1.000000       1.00000       1.00000      1.000000   \n",
      "\n",
      "       Soil_Type_37  Soil_Type_38  Soil_Type_39  Soil_Type_40    Cover_Type  \n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000  \n",
      "mean       0.000500      0.026800      0.024500      0.013600      2.036600  \n",
      "std        0.022356      0.161507      0.154603      0.115829      1.383782  \n",
      "min        0.000000      0.000000      0.000000      0.000000      1.000000  \n",
      "25%        0.000000      0.000000      0.000000      0.000000      1.000000  \n",
      "50%        0.000000      0.000000      0.000000      0.000000      2.000000  \n",
      "75%        0.000000      0.000000      0.000000      0.000000      2.000000  \n",
      "max        1.000000      1.000000      1.000000      1.000000      7.000000  \n",
      "\n",
      "[8 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "# Summarize data\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see anything that would require scaling of the data?  If so, apply that transformation.\n",
    "\n",
    "Yes, some of the features (like distances versus soil type) have different scales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define features and target\n",
    "X = df.drop(columns=['Cover_Type'])\n",
    "y = df['Cover_Type'] \n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=808)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Hyperparameter Tuning for SVM\n",
    "To optimize our SVM model, we need to search for the best hyperparameters that maximize classification accuracy. Since SVM performance depends heavily on `C`, `kernel`, and `gamma`, we will use `GridSearchCV()` to systematically test different combinations. Initialize a cross validation object with 5 folds using `StratifiedKFold`. Check out how `StratifiedKFold` differs from `Kfold` [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html). \n",
    "\n",
    "Then, set up a grid to test different values of: \n",
    "- `C` (regularization strength): how strictly the model fits the training data\n",
    "  - Candidate parameter values: `(0.1, 1, 10, 100)`\n",
    "- `kernel` (decision boundary shape): compares linear and radial basis function shapes\n",
    "  - Candidate parameter values: (linear, rbf)\n",
    "- `gamma` (influence of training observations): influence of individual points on decision boundary\n",
    "  - Candidate parameter values: (scale, auto)\n",
    "\n",
    "As models and datasets become more complex, consideration of computation time becomes more important.  You'll use `time.time()` to measure the time required to fit the grid object.  \n",
    "\n",
    "**Print the best parameters from your model, as well as the time required to fit the grid object.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "svm = SVC()\n",
    "\n",
    "# Initialize cv with stratified kfold\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],             \n",
    "    'kernel': ['linear', 'rbf'],       \n",
    "    'gamma': ['scale', 'auto']}\n",
    "\n",
    "# different combos of C, Kernel, and gamma\n",
    "grid_search_svm = GridSearchCV(svm, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Measure time \n",
    "start_time = time.time()\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters for SVM:\", grid_search_svm.best_params_)\n",
    "print(f\"Time to fit GridSearchCV for SVM: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build and fit a Random Forest for comparison\n",
    "\n",
    "Let's compare our SVM to a Random Forest classifier.  Create a grid for cross-validation with three hyperparameters of your choice to tune, along with three sensible values for each one.  \n",
    "**Print the best parameters from your model, as well as the time required to fit the grid object.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=808)\n",
    "\n",
    "# Define hyperparameter grid, used lab 6 for reference\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# Perform gridsearch cv\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Measure time \n",
    "start_time = time.time()\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the best parameters \n",
    "print(\"Best Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "print(f\"Time to fit GridSearchCV: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model Predictions and Evaluation\n",
    "Now that you have trained and optimized both a SVM and RF model, you will evaluate their performances on the test set to prepare for model comparison. In this step, you will:\n",
    "- Use the best models from `GridSearchCV()` to make predictions on the test set\n",
    "- Generate a confusion matrix for each model to visualize classification performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best models\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_svm = best_svm.predict(X_test)\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# make confusion matrix for each model\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make confusion matrix for SVM\n",
    "# Initialize figure\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Plot SVM confusion matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm_svm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - SVM\")\n",
    "\n",
    "# Plot Random Forest confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap=\"Greens\", cbar=False)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Gather and display additional performance metrics\n",
    "Now display the accuracy score and training time required for each model to so we can compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")\n",
    "\n",
    "# Calculate SVM training time\n",
    "start_time = time.time()\n",
    "best_svm.fit(X_train, y_train)\n",
    "svm_training_time = time.time() - start_time\n",
    "\n",
    "# Random Forest training time\n",
    "start_time = time.time()\n",
    "best_rf.fit(X_train, y_train)\n",
    "rf_training_time = time.time() - start_time\n",
    "\n",
    "# Print training times\n",
    "print(f\"SVM Training Time (final model): {svm_training_time:.2f} seconds\")\n",
    "print(f\"Random Forest Training Time (final model): {rf_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Compare the models\n",
    "Now that we have trained, optimized, and evaluated both SVM and RF models, we will compare them based on overall accuracy, training time, and types of errors made.\n",
    "\n",
    "Based on these comparisons, which model is more suitable for this task?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the numbers above, the random forest model outperforms the SVM model in accuracy and training time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 3 (EDS232)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
